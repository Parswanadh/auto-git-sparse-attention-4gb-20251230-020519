# Sparse Attention for 4GB VRAM

Efficient block-diagonal sparse attention optimized for 4GB VRAM constraint.

## Features
- O(n) block-diagonal attention pattern
- Works with sequences up to 4096 tokens on 4GB GPU
- ~10M parameters

## Usage
```python
from model import SparseTransformer

model = SparseTransformer()
# model.fit(...)
```

Generated by Auto-GIT: 2025-12-30 02:05:19
